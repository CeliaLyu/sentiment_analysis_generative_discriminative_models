{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b00dab3-3218-4cd5-a385-d8caf5fdd4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jz288/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_imdb, get_train_test_split\n",
    "\n",
    "import os, sys, random, functools\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a02044-a288-4840-96a9-a592034d0c4b",
   "metadata": {},
   "source": [
    "# load raw data and split it into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01885126-2c4f-41de-9a7d-3705d809b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train data is 40000\n",
      "length of test data is 10000\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = get_train_test_split(\"IMDBDataset.csv\", test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17504aa0-8999-439d-9103-fbef14f25a15",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8acca9-981e-4388-8699-173cb228444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(x_train, min_freq=1, hparams=None):\n",
    "    word_list = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "\n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
    "    # creating a dict\n",
    "    # here assume the pad token is 0 and unknown token is 1\n",
    "    vocab = {w: i+2 for i, w in enumerate(corpus_)}\n",
    "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
    "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
    "    return vocab\n",
    "\n",
    "def tokenize(vocab, sentence):\n",
    "    return [vocab[word] for word in sentence.lower().split() if word in vocab.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab385d-575c-4e80-8c2c-3de0e797f54d",
   "metadata": {},
   "source": [
    "# a custom pytorch dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c396ab9b-89fc-47f3-b280-b883aaa1871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDB(Dataset):\n",
    "    def __init__(self, x, y, vocab, max_length=256) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_ids = tokenize(self.vocab, self.x[idx])\n",
    "        if self.max_length:\n",
    "            token_ids = token_ids[:self.max_length]\n",
    "\n",
    "        label = 1 if self.y[idx] == 'positive' else 0\n",
    "        return {\"ids\": token_ids, \"length\": len(token_ids), \"label\": label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc144467-5612-42f0-bd6c-5194bbb0e005",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765d4cb3-499c-41e0-a236-479614a012c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        embedding_dim: int, \n",
    "        hidden_dim: int, \n",
    "        output_dim: int, \n",
    "        n_layers: int, \n",
    "        dropout_rate: float, \n",
    "        pad_index: int,\n",
    "        bidirectional: bool,\n",
    "        **kwargs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_rate, batch_first=True,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True,\n",
    "                                                            enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        prediction = self.fc(hidden)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2abe9-a8eb-4f2d-b429-395f2bf45dde",
   "metadata": {},
   "source": [
    "# training/evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d383f6-d935-4fd7-ae55-c5fcaa68813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        ids = batch['ids'].to(device)\n",
    "        length = batch['length']\n",
    "        label = batch['label'].to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_acc += get_correct(prediction, label)\n",
    "        scheduler.step()\n",
    "\n",
    "    return epoch_losses, epoch_acc / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
    "            ids = batch['ids'].to(device)\n",
    "            length = batch['length']\n",
    "            label = batch['label'].to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_acc += get_correct(prediction, label)\n",
    "\n",
    "    return epoch_losses, epoch_acc / len(dataloader.dataset)\n",
    "\n",
    "def get_correct(prediction, label):\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    return predicted_classes.eq(label).sum().item()\n",
    "\n",
    "def predict_sentiment(text, model, vocab, device):\n",
    "    tokens = tokenize(vocab, text)\n",
    "    ids = [vocab[t] if t in vocab else UNK_INDEX for t in tokens]\n",
    "    length = torch.LongTensor([len(ids)])\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    prediction = model(tensor, length).squeeze(dim=0)\n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "def collate(batch, pad_index):\n",
    "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
    "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
    "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
    "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
    "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
    "    return batch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        num_warmup_steps: int,\n",
    "    ):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self._step_count <= self.num_warmup_steps:\n",
    "            # warmup\n",
    "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
    "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        else:\n",
    "            lr = self.base_lrs\n",
    "        return lr\n",
    "\n",
    "def train_and_test_model_with_hparams(data, hparams, model_type=\"lstm\", **kwargs):\n",
    "    torch.manual_seed(hparams.SEED)\n",
    "    random.seed(hparams.SEED)\n",
    "    np.random.seed(hparams.SEED)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = data\n",
    "    vocab = build_vocab(x_train, hparams=hparams)\n",
    "    vocab_size = len(vocab)\n",
    "    print(f'Length of vocabulary is {vocab_size}')\n",
    "\n",
    "    train_data = IMDB(x_train, y_train, vocab, hparams.MAX_LENGTH)\n",
    "    test_data = IMDB(x_test, y_test, vocab, hparams.MAX_LENGTH)\n",
    "\n",
    "    collate_fn = functools.partial(collate, pad_index=hparams.PAD_INDEX)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model\n",
    "    model = LSTM(\n",
    "        vocab_size, \n",
    "        hparams.EMBEDDING_DIM, \n",
    "        hparams.HIDDEN_DIM, \n",
    "        hparams.OUTPUT_DIM,\n",
    "        hparams.N_LAYERS,\n",
    "        hparams.DROPOUT_RATE, \n",
    "        hparams.PAD_INDEX,\n",
    "        hparams.BIDIRECTIONAL,\n",
    "        **kwargs)\n",
    "    num_params = count_parameters(model)\n",
    "    print(f'The model has {num_params:,} trainable parameters')\n",
    "    print('='*50)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.RMSprop(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Start training\n",
    "    best_test_loss = float('inf')\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    # Warmup Scheduler\n",
    "    WARMUP_STEPS = 200\n",
    "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
    "\n",
    "    best_acc = 0.\n",
    "    for epoch in range(hparams.N_EPOCHS):\n",
    "        train_losses, train_acc = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
    "        test_losses, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "        \n",
    "        if test_acc >= best_acc:\n",
    "            best_acc = test_acc\n",
    "            #torch.save(\n",
    "            #    model.state_dict(),\n",
    "            #    'best_lstm.pth'\n",
    "            #)\n",
    "\n",
    "        tqdm.write(f'epoch: {epoch+1}')\n",
    "        tqdm.write(f'train_loss: {np.mean(train_losses):.3f}, train_acc: {train_acc:.3f}')\n",
    "        tqdm.write(f'test_loss: {np.mean(test_losses):.3f}, test_acc: {test_acc:.3f}')\n",
    "        tqdm.write('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea319e10-399f-4742-8a2f-f462d3f686b8",
   "metadata": {},
   "source": [
    "# training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d133630-8dce-4659-84a8-5c73f5579f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams:\n",
    "    def __init__(self):\n",
    "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
    "        self.PAD_INDEX = 0\n",
    "        self.UNK_INDEX = 1\n",
    "        self.PAD_TOKEN = '<pad>'\n",
    "        self.UNK_TOKEN = '<unk>'\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        self.MAX_LENGTH = 256\n",
    "        self.BATCH_SIZE = 96\n",
    "        self.EMBEDDING_DIM = 1\n",
    "        self.HIDDEN_DIM = 100\n",
    "        self.OUTPUT_DIM = 2\n",
    "        self.N_LAYERS = 1\n",
    "        self.DROPOUT_RATE = 0\n",
    "        self.LR = 0.001\n",
    "        self.N_EPOCHS = 5\n",
    "        self.WD = 0\n",
    "        self.BIDIRECTIONAL = False\n",
    "        self.SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ec3f5c-6ab6-4777-9689-fa98658a96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 318916\n",
      "The model has 360,318 trainable parameters\n",
      "==================================================\n",
      "training...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 417/417 [00:15<00:00, 26.58it/s]\n",
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:01<00:00, 61.51it/s]\n",
      "epoch: 1\n",
      "train_loss: 0.544, train_acc: 0.711\n",
      "test_loss: 0.428, test_acc: 0.815\n",
      "--------------------------------------------------\n",
      "training...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 417/417 [00:15<00:00, 27.04it/s]\n",
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:01<00:00, 58.62it/s]\n",
      "epoch: 2\n",
      "train_loss: 0.376, train_acc: 0.841\n",
      "test_loss: 0.352, test_acc: 0.857\n",
      "--------------------------------------------------\n",
      "training...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 417/417 [00:15<00:00, 26.95it/s]\n",
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:01<00:00, 61.30it/s]\n",
      "epoch: 3\n",
      "train_loss: 0.259, train_acc: 0.900\n",
      "test_loss: 0.382, test_acc: 0.826\n",
      "--------------------------------------------------\n",
      "training...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 417/417 [00:15<00:00, 26.97it/s]\n",
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:01<00:00, 62.24it/s]\n",
      "epoch: 4\n",
      "train_loss: 0.206, train_acc: 0.924\n",
      "test_loss: 0.297, test_acc: 0.875\n",
      "--------------------------------------------------\n",
      "training...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 417/417 [00:15<00:00, 26.75it/s]\n",
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:01<00:00, 60.92it/s]\n",
      "epoch: 5\n",
      "train_loss: 0.173, train_acc: 0.937\n",
      "test_loss: 0.309, test_acc: 0.885\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hparams = HyperParams()\n",
    "hparams.N_LAYERS = 1\n",
    "hparams.DROPOUT_RATE = 0.\n",
    "hparams.WD = 0.0001\n",
    "hparams.BIDIRECTIONAL = False\n",
    "hparams.MAX_LENGTH = 512\n",
    "train_and_test_model_with_hparams([x_train, x_test, y_train, y_test], hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed32681-5fcb-4f21-815a-4fa769304b70",
   "metadata": {},
   "source": [
    "|embedding dim |layers |hidden dim |bidirectional |parameters |dropout rate |wd |epochs |train acc |test acc |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|1 |1 |100 |False |360,318 |0 |0 |5 |0.992 |0.874 |\n",
    "|1 |1 |100 |False |360,318 |0 |0 |10 |0.988 |0.833 |\n",
    "|5 |1 |50 |False |1,606,082 |0 |0 |5 |0.994 |0.873 |\n",
    "|1 |1 |100 |False |360,318 |0.1 |0 |5 |0.989 |0.876 |\n",
    "|1 |1 |100 |False |360,318 |0.2 |0 |5 |0.851 |0.885 |\n",
    "|1 |1 |100 |False |360,318 |0.3 |0 |5 |0.949 |0.888 |\n",
    "|1 |1 |100 |False |360,318 |0 |5e-5 |5 |0.941 |0.882 |\n",
    "|1 |1 |100 |False |360,318 |0 |1e-4 |5 |0.922 |0.891 |\n",
    "|1 |1 |100 |False |360,318 |0 |5e-4 |5 |0.800 |0.733 |\n",
    "|1 |1 |100 |False |360,318 |0.1 |1e-4 |5 |0.916 |0.888 |\n",
    "|1 |1 |100 |False |360,318 |0.3 |1e-4 |5 |0.895 |0.887 |\n",
    "|1 |2 |100 |False |441,118 |0 |1e-4 |5 |0.900 |0.879 |\n",
    "|1 |1 |100 |True |401,518 |0 |1e-4 |5 |0.861 |0.881 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0960ae9-f64f-4dea-822d-52ca1c95c856",
   "metadata": {},
   "source": [
    "# evaluate on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56328bd-7579-483e-8b0e-6faf249e72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'ngram' # bayes\n",
    "reviews, sentiments = load_imdb(f\"synthetic_data/{method}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8bfa3d6-e4e9-4e23-91dd-651e5b8292e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_acc = 0\n",
    "    correct = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
    "            ids = batch['ids'].to(device)\n",
    "            length = batch['length']\n",
    "            label = batch['label'].to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_acc += get_correct(prediction, label)\n",
    "            correct.append(\n",
    "                prediction.argmax(1).eq(label).cpu()\n",
    "            )\n",
    "\n",
    "    return epoch_losses, epoch_acc / len(dataloader.dataset), torch.cat(correct)\n",
    "\n",
    "\n",
    "def test_model_with_hparams(x_train, x_test_syn, y_test_syn, hparams, model_ckpt_path, model_type=\"lstm\", **kwargs):\n",
    "    vocab = build_vocab(x_train, hparams=hparams)\n",
    "    vocab_size = len(vocab)\n",
    "    print(f'Length of vocabulary is {vocab_size}')\n",
    "\n",
    "    test_data = IMDB(x_test_syn, y_test_syn, vocab, hparams.MAX_LENGTH)\n",
    "\n",
    "    collate_fn = functools.partial(collate, pad_index=hparams.PAD_INDEX)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = LSTM(\n",
    "        vocab_size, \n",
    "        hparams.EMBEDDING_DIM, \n",
    "        hparams.HIDDEN_DIM, \n",
    "        hparams.OUTPUT_DIM,\n",
    "        hparams.N_LAYERS,\n",
    "        hparams.DROPOUT_RATE, \n",
    "        hparams.PAD_INDEX,\n",
    "        hparams.BIDIRECTIONAL,\n",
    "        **kwargs)\n",
    "    num_params = count_parameters(model)\n",
    "    print(f'The model has {num_params:,} trainable parameters')\n",
    "    print('='*50)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_ckpt_path))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_losses, test_acc, test_correct = evaluate(test_dataloader, model, criterion, device)\n",
    "    print(f'test_acc: {test_acc:.3f}')\n",
    "    return test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972e1c89-85d9-438f-86b6-ef609df3999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 318916\n",
      "The model has 360,318 trainable parameters\n",
      "==================================================\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 190.21it/s]\n",
      "test_acc: 0.797\n"
     ]
    }
   ],
   "source": [
    "hparams = HyperParams()\n",
    "hparams.N_LAYERS = 1\n",
    "hparams.DROPOUT_RATE = 0.\n",
    "hparams.WD = 0.0001\n",
    "hparams.BIDIRECTIONAL = False\n",
    "correct = test_model_with_hparams(\n",
    "    x_train, reviews, sentiments, hparams, \n",
    "    'best_lstm.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3e5d5-b2d3-49f0-b6cd-bd311730132e",
   "metadata": {},
   "source": [
    "# qualitative examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e9c0809-05bb-4473-b196-5193c862a58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I found it real shocking at first to see William Shakespeare's love masterpiece reworked into a gory, violent and kinky sensual movie adaptation. But after you watched it once, it sort of grows on you when you watch it the second and third times, as you come over the shock and start appreciating the movie on its own merits - solid acting, good dialogue, nice sequencing and choreography, not-too-bad soundtrack and some of the (special) effects that go on. Oh, and also the ending. What a riot! eos\",\n",
       "       \"The beginning of this movie had me doubting that it would be little more than a typical B sci-fi flick. But, as it progressed I began to get interested and I saw the whole thing through. The premise is interesting, original, and has the makings of making a classic. Alas, it instead ended up a mediocre movie, done in by the usual factors which turn a potentially good movie into a bad movie (bad acting, low budget etc.). I'm interested to see how this would turn out if it were remade with good actors and a big hollywood budget. eos\",\n",
       "       \"This movie is a disgrace to the Major League Franchise. I live in Minnesota and even I can't believe they dumped Cleveland. (Yes I realize at the time the real Indians were pretty good, and the Twins had taken over their spot at the bottom of the American League, but still be consistent.) Anyway I loved the first Major League, liked the second, and always looked forward to the third, when the Indians would finally go all the way to the series. You can't tell me this wasn't the plan after the second film was completed. What Happened? Anyways if your a true fan of the original Major League do yourself a favor and don't watch this junk. eos\"],\n",
       "      dtype='<U8000')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.array([10,20,30])\n",
    "\n",
    "num = 5\n",
    "np.array(x_test)[correct.numpy().nonzero()[0][indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75740346-a0c6-4391-bc07-1b6ddb41c786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test)[correct.numpy().nonzero()[0][indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28626a3f-4288-4886-92c0-3615949e26ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"This movie is beautiful in all ways. It is visually stunning, and this is a good thing since the dialogue would only take up a page or two of paper. The acting is superb; it is subtle, passionate and intense. Ben Daniels does a fabulous job of turning himself into an animal, and mixing that wild nature with a man's overbearing passion and honor. There is not one flaw, not one mistake or wrong moment to be found anywhere. It is completely perfect, but only if you understand what you're going to experience. It isn't a movie for anyone who wants normality. eos\",\n",
       "       \"I thought the film could be a bit more complex,in a psychological sense perhaps, but the action and voice acting were top notch. The animation was heavy CG in many scenes, but very good ones at that. This is one of the Batman Returns/Forever type films, which include romances and the conflicts of Wayne and motives for dating. 007 fans would love this, and so would the females, great theme song! Wayne was portrayed very well in this film, and the Penquin was back to his true form, no mutant genes in him this time! I liked the fact Robin wasn't used too much, Tim Drake was just a good computer nerd, somewhat of an Indigo child or mind of the future.  The supporting cast was made up of some soap opera stars, decent talents and the characters were drawn to look like the voice actors too. Kelly Ripa was hilarious in this film.  I rate this below Phantasm, Return of the Joker, and Batman vs. Dracula, but liked the smarter script better than I enjoyed Subzero. 7/10 eos\",\n",
       "       \"As it is often the case, the impressive and explosive trailers of Asian films add up to nothing more than lackluster stories. Similar to Unleashed (which was great,) Dog Bite Dog tells a story where men are raised as ferocious savage dogs that carry out their master's bidding. The main characters, an emotionally undeveloped, amoral killer who is matched against an equally unstable police officer, are far from the common heroes and villains we often see. In fact, by the end, you lose track of who you're supposed to empathize with, failing to feel even the slightest emotion for either of the men \\x96 whether that was the failure of the director or perhaps the underlining message he was trying to tell is up to you to decide.  Although the beginning of the film was filled with intrigue and unpredictability, by the half-way point it slopped down to a humdrum story of survival and revenge. The suspense which was evident at first soon disappeared because of a grossly mismatched music score which brought down the potentially effective story telling. And in the end, you were left feeling that all that detailed background information and introspection of the main characters was somehow very unnecessary.  On the plus side, the transition in story from point a to point b was quite atypical compared to US movies \\x96 so those who aren't familiar with Asian films and are tired of Hollywood's predictability should check it out.  The white balance seemed off throughout most of the film. It was like looking into a picture shot on fluorescent when it was supposed to be set on tungsten. Maybe I'm the only one, but it strained my eyes.  The movie also enjoyed playing tricks on you \\x96 an interesting build-up gave me hope for the slow moving story until it was diverted to a low budget, low speed chase scene. And just when you think you were going to get an unanswered indie ending with a mix of Shakespearean tragedy, you realize that it's not an ending at all, but rather a transition into a wacky country-music montage about peace and serenity.  Throw in some grisly from-behind choke scenes, a moment of redemption unexpectedly brought back into savagery and back again the other way, Asians' fascination with bodily fluids and a horrible music score that didn't match the film, and you get the average bland Asian thriller.  I just don't get why every fight scene was overlaid with clips of roaring lions \\x85I thought they were supposed to symbolize dogs? Ultimately, in the end, we are reminded about a true killer that still lurks amongst us \\x96 tetanus.  4/10 eos\",\n",
       "       \"Sadly it was misguided. This movie stunk from start to finish. It was hard to watch because I used to watch Clarissa Explains It All every day on Nickelodeon. I LOVED her. Then the next thing I found she did a spread in Maxim and she was gorgeous! I haven't really heard anything about her until I watched this movie on accident. I couldn't believe she would even let something like this be seen with her name all over it. Everything about it was wrong but it still looked like someone somewhere in the team was trying really really hard to save a sunk ship. Too bad.. I hope she continues to act and I would love to see her with a real cast in a real movie. eos\",\n",
       "       'Cheesy script, cheesy one-liners. Timothy Hutton\\'s performance a \"little\" over the top. David Duchovny still seemed to be stuck in his Fox Mulder mode. No chemistry with his large-lipped female co-star.He needs Gillian Anderson to shine. He does not seem to have any talent of his own. eos'],\n",
       "      dtype='<U8000')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_test)[1-correct.numpy().nonzero()[0][:num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "700274ed-031d-4a6b-98a8-80d5f0a7ae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'negative', 'negative', 'negative'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test)[1-correct.numpy().nonzero()[0][:num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4a038d-7495-476f-a41b-4a2a314f00d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 318916\n",
      "The model has 360,318 trainable parameters\n",
      "==================================================\n",
      "evaluating...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 342.81it/s]\n",
      "test_acc: 0.355\n"
     ]
    }
   ],
   "source": [
    "hparams = HyperParams()\n",
    "hparams.N_LAYERS = 1\n",
    "hparams.DROPOUT_RATE = 0.\n",
    "hparams.WD = 0.0001\n",
    "hparams.BIDIRECTIONAL = False\n",
    "test_model_with_hparams(\n",
    "    x_train, reviews, sentiments, hparams, \n",
    "    'best_lstm.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00192a14-0c99-47d7-a5fd-1fd1c1752fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
