{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "1f664568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lyuxiaoxi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import io, sys, math, re\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae6487",
   "metadata": {},
   "source": [
    "# load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "22d8d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb(base_csv, test_size):\n",
    "    \"\"\"\n",
    "    Load the IMDB dataset\n",
    "    :param base_csv: the path of the dataset file.\n",
    "    :return: train and test set.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(base_csv, engine='python')\n",
    "    df.head()\n",
    "    reviews, sentiments = df['review'].values, df['sentiment'].values\n",
    "    \n",
    "    # we are adding an end of sentence word(eos) at the end of the sentence to help\n",
    "    #    us know when the sentence ends when generating a sentence\n",
    "    reviews=[i+\" eos\" for i in reviews]\n",
    "    \n",
    "    # remove useless characters '<br />'\n",
    "    for i, element in enumerate(reviews):\n",
    "        element = element.replace('<br />',' ')\n",
    "        reviews[i] = element\n",
    "    \n",
    "    return reviews, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d54a48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train data is 40000\n",
      "length of test data is 10000\n"
     ]
    }
   ],
   "source": [
    "base_csv='IMDBDataset.csv'\n",
    "test_size=0.2\n",
    "reviews, sentiments = load_imdb(base_csv, test_size)\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=test_size, stratify=sentiments, random_state=233)\n",
    "print(f'length of train data is {len(x_train)}')\n",
    "print(f'length of test data is {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "d58c0cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man, I really love the new DVD that Universal put out. I've never seen THE SENTINEL look this good since I had to put up with crappy, grainy VHS tapes for years. Unfortunately there are no extras beyond a trailer that looks pretty worse for wear. And AVOID the Goodtimes DVD at all costs. It sucks.  Anyway, troubled fashion model Alison Parker (Cristina Raines) moves into haunted NYC brownstone, only it's more than just haunted. It's also a portal to hell and the Vatican keeps an old blind priest (John Carradine) to keep watch over it and make sure the devils and arch-angels don't escape.   This has an all star cast full of old-timey actors like Ava Gardner, Arthur Kennedy, Jose Ferrer etc... as well as cameos of upcoming 80s stars including Christopher Walken, Jeff Goldblum (who's voice was mysteriously overdubbed) and Tom Berenger. And you won't even recognize Jerry Orbach from LAW & ORDER. I had to do a double-take when I didn't quite place where I'd seen him before.  Nice gore scenes of Alison slicing the eye and nose off her dead father's rotting corpse that's been possessed by the devil. And there's a neat ending where disfigured, deformed people try to haunt Alison into committing suicide so she won't be the next one to guard the portal. It seems Alison's troubled past makes her a prime candidate by the Vatican to become the next sentinel.  An excellent, creepy 70s classic from director Michael Winner that shouldn't be missed. I also recommended it for those who want something a little more imaginative beyond the usual stupid teenager slashers and horror comedy.  7 out of 10  - eos\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0162d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2344c2",
   "metadata": {},
   "source": [
    "##  Step 2: \n",
    "### Identify or construct a solution based on a generative probabilistic (language) model. Describe the model in detail and develop a solution using parameter inference (and/or decoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0e76a",
   "metadata": {},
   "source": [
    "### Method 1: N-gram for sentiment analysis\n",
    "\n",
    "Codes reference link: \n",
    "- https://github.com/siddiquiamir/NLTK-Text-Mining\n",
    "- https://www.youtube.com/watch?v=ic5XcUxiUrI\n",
    "\n",
    "N-gram concepts (for report writing):\n",
    "- https://www.cnblogs.com/bep-feijin/p/9430164.html\n",
    "\n",
    "CountVectorize: \n",
    "- https://blog.csdn.net/weixin_38278334/article/details/82320307\n",
    "- https://blog.csdn.net/appleyuchi/article/details/108261497\n",
    "- https://www.biaodianfu.com/sklearn-feature-extraction-text.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318dbdf6",
   "metadata": {},
   "source": [
    "#### Convert the dataset into a matrix \n",
    "p.s. parameters in function CountVectorizer will be tuned in later steps, such as ngram_range..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "641c32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Try different N in N-gram model\n",
    "\"\"\"\n",
    "\n",
    "x_train_vectorized = []\n",
    "vect = []\n",
    "\n",
    "vect_1 = CountVectorizer(min_df=5, ngram_range=(1,1)).fit(x_train)\n",
    "x_train_vectorized_1 = vect_1.transform(x_train)\n",
    "x_train_vectorized.append(x_train_vectorized_1)\n",
    "vect.append(vect_1)\n",
    "\n",
    "vect_2 = CountVectorizer(min_df=5, ngram_range=(2,2)).fit(x_train)\n",
    "x_train_vectorized_2 = vect_2.transform(x_train)\n",
    "x_train_vectorized.append(x_train_vectorized_2)\n",
    "vect.append(vect_2)\n",
    "\n",
    "vect_3 = CountVectorizer(min_df=5, ngram_range=(3,3)).fit(x_train)\n",
    "x_train_vectorized_3 = vect_3.transform(x_train)\n",
    "x_train_vectorized.append(x_train_vectorized_3)\n",
    "vect.append(vect_3)\n",
    "\n",
    "vect_4 = CountVectorizer(min_df=5, ngram_range=(4,4)).fit(x_train)\n",
    "x_train_vectorized_4 = vect_4.transform(x_train)\n",
    "x_train_vectorized.append(x_train_vectorized_4)\n",
    "vect.append(vect_4)\n",
    "\n",
    "vect_5 = CountVectorizer(min_df=5, ngram_range=(5,5)).fit(x_train)\n",
    "x_train_vectorized_5 = vect_5.transform(x_train)\n",
    "x_train_vectorized.append(x_train_vectorized_5)\n",
    "vect.append(vect_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a5718",
   "metadata": {},
   "source": [
    "#### Train a logistic model with different N (from 1 to 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "eaf3cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyuxiaoxi/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/lyuxiaoxi/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/lyuxiaoxi/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "prediction_lis = []\n",
    "model = []\n",
    "for i in range(5):\n",
    "    m = LogisticRegression()\n",
    "    m.fit(x_train_vectorized[i], y_train)\n",
    "    predictions = m.predict(vect[i].transform(x_test))\n",
    "    prediction_lis.append(predictions)\n",
    "    model.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6154309",
   "metadata": {},
   "source": [
    "#### Model evaluation with different N (from 1 to 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "8f618407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score is 0.8867999999999999\n",
      "AUC score is 0.8876\n",
      "AUC score is 0.8503\n",
      "AUC score is 0.7843\n",
      "AUC score is 0.6868\n"
     ]
    }
   ],
   "source": [
    "auc_lis = []\n",
    "for j in range(5):\n",
    "    for i in range(len(prediction_lis[j])):\n",
    "        if prediction_lis[j][i]=='positive':\n",
    "            prediction_lis[j][i]=1\n",
    "        elif prediction_lis[j][i]=='negative':\n",
    "            prediction_lis[j][i]=0\n",
    "    for i in y_test:\n",
    "        if i=='positive':\n",
    "            i=1\n",
    "        elif i=='negative':\n",
    "            i=0      \n",
    "    auc = roc_auc_score(y_test, prediction_lis[j])\n",
    "    print(\"AUC score is\", auc)\n",
    "    auc_lis.append(auc)\n",
    "    # feature_names = np.array(vect[j].get_feature_names())\n",
    "    # sorted_coef_index = model[j].coef_[0].argsort()\n",
    "    # print(\"Negative words\", feature_names[sorted_coef_index[:10]])\n",
    "    # print(\"Positive words\", feature_names[sorted_coef_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7526fc3",
   "metadata": {},
   "source": [
    "### -> We find that when N=2 (i.e. bigram), prediction accuracy using Logistic Regression is the highest; when N=1 or 2 or 3, prediction accuracy are all pretty high (>85%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff0e00",
   "metadata": {},
   "source": [
    "## Step 5: Train and apply both approaches to synthetic data that you generate according to the generative model from Step 2. Evaluate the results qualitatively and quantitatively. Highlight situations where each approach performs well and poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77943b8d",
   "metadata": {},
   "source": [
    "## Method : N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "caf82a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preparation\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('IMDBDataset.csv', engine='python')\n",
    "df_pos = df[df['sentiment'] =='positive']\n",
    "df_neg = df[df['sentiment'] =='negative']\n",
    "x_pos, y_pos = df_pos['review'].values, df_pos['sentiment'].values\n",
    "# we are adding an end of sentence word(eos) at the end of the sentence to help\n",
    "#    us know when the sentence ends when generating a sentence\n",
    "x_pos = [i+\" eos\" for i in x_pos]\n",
    "# remove useless characters \n",
    "for i, element in enumerate(x_pos):\n",
    "    element = element.replace('<br />',' ')\n",
    "    element = element.replace('.',' ')\n",
    "    element = element.replace(',',' ')\n",
    "    element = element.replace('!',' ')\n",
    "    element = element.replace('...',' ')\n",
    "    element = element.replace('(',' ')\n",
    "    element = element.replace(')',' ')\n",
    "    element = element.replace(':',' ')                    \n",
    "    x_pos[i] = element\n",
    "x_neg, y_neg = df_neg['review'].values, df_neg['sentiment'].values\n",
    "x_neg = [i+\" eos\" for i in x_neg]\n",
    "for i, element in enumerate(x_neg):\n",
    "    element = element.replace('<br />',' ')\n",
    "    element = element.replace('.',' ')\n",
    "    element = element.replace(',',' ')\n",
    "    element = element.replace('!',' ')\n",
    "    element = element.replace('...',' ')\n",
    "    element = element.replace('(',' ')\n",
    "    element = element.replace(')',' ')\n",
    "    element = element.replace(':',' ')\n",
    "    x_neg[i] = element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b63e4",
   "metadata": {},
   "source": [
    "#### Implement the N-Gram Text Generator model \n",
    "Q: How does the N-Gram Text Generator model work?\n",
    "\n",
    "A: The logic behind the model is the following – given two words, we check for Tri-Grams that start with these words and the algorithm picks one based on its probability. \n",
    "\n",
    "For example, let’s say we have the words “if” and “you”, and there are the following trigrams that start with them along with their counts “if you agree”:25 and “if you believe”:5. The algorithm will pick one of these two based on the probabilities 25/30 for the first one and 5/30 for the second one. This will continue until we have the word “eos” as our next word. \n",
    "\n",
    "### To deal with rare n-grams (i.e., the n-gram is not in the original dataset), generally we have 3 options - smoothing, backoff, and interpolation. We will try some of them as follows. \n",
    "### 1. Kneser Ney Smoothing\n",
    "reference list:\n",
    "- https://stackoverflow.com/questions/43939344/compute-ngrams-across-a-list-of-lists-of-sentences-using-nltk\n",
    "- https://stackoverflow.com/questions/61761215/how-to-perform-kneser-ney-smoothing-in-nltk-at-word-level-for-bigram-language-mo\n",
    "- https://tedboy.github.io/nlps/generated/generated/nltk.KneserNeyProbDist.html\n",
    "- https://www.nltk.org/api/nltk.probability.CrossValidationProbDist.html?highlight=probability+probdist#nltk.probability.CrossValidationProbDist.samples\n",
    "- https://predictivehacks.com/text-generation-for-instagram-using-n-grams/\n",
    "- https://practicaldatascience.co.uk/machine-learning/how-to-use-count-vectorization-for-n-gram-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7ccedf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We choose 3-gram in this case, since n must be 3 in function nltk.KneserNeyProbDist().\n",
    "\"\"\"\n",
    "\n",
    "n=3 \n",
    "freq_dist_pos = nltk.FreqDist() \n",
    "for sent in x_pos:\n",
    "    freq_dist_pos.update(nltk.ngrams(sent.split(), n))\n",
    "kneser_ney_pos = nltk.KneserNeyProbDist(freq_dist_pos)\n",
    "freq_dist_neg = nltk.FreqDist() \n",
    "for sent in x_neg:\n",
    "    freq_dist_neg.update(nltk.ngrams(sent.split(), n))\n",
    "kneser_ney_neg = nltk.KneserNeyProbDist(freq_dist_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "e0ddb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic data (i.e., 20 new reviews) according to the generative model in Step 2.\n",
    "\"\"\"\n",
    "\n",
    "def word_gen(start_word, kneser_ney):\n",
    "    # initialize list_words as start_word\n",
    "    list_words = []\n",
    "    list_words.append(start_word[0])\n",
    "    list_words.append(start_word[1])\n",
    "    # we make constraint that length of each generated review should be limited within 200 words\n",
    "    while (list_words[-1]!='eos' and len(list_words)<=100): \n",
    "        # stochastic: randomly choose a word\n",
    "        words_freq = dict()\n",
    "        for i in kneser_ney.samples():\n",
    "            if (i[0]==list_words[-2] and i[1]==list_words[-1]):\n",
    "                words_freq[i] = kneser_ney.prob(i) \n",
    "        # if the gram is NOT in kneser_ney, add the start word into this sentence.\n",
    "        if (len(words_freq)==0):\n",
    "            for i in kneser_ney.samples():\n",
    "                if (i[0]==start_word[0] and i[1]==start_word[1]):\n",
    "                    words_freq[i] = kneser_ney.prob(i) \n",
    "        # random select a gram, according to the freq of each gram\n",
    "        word = random.choices(list(words_freq.keys()), weights=words_freq.values(), k=1)\n",
    "        list_words.append(word[0][-2])  \n",
    "        list_words.append(word[0][-1])  \n",
    "    # generate a sentence\n",
    "    generated_sentence = ' '.join(list_words[:-1])\n",
    "    return generated_sentence\n",
    "\n",
    "def generate_synthetic_datasets(freq_dist, kneser_ney):\n",
    "    freq_dist_most_common = freq_dist.most_common(10) # generate 20 new reviews\n",
    "    lis_generated_sentences = []\n",
    "    for i in freq_dist_most_common:\n",
    "        start_word = i[0]\n",
    "        start_word = start_word[0:2]\n",
    "        generated_sentence = word_gen(start_word, kneser_ney)\n",
    "        lis_generated_sentences.append(generated_sentence)\n",
    "    return lis_generated_sentences\n",
    "\n",
    "lis_generated_sentence_pos = generate_synthetic_datasets(freq_dist_pos, kneser_ney_pos)\n",
    "generated_review_pos = lis_generated_sentence_pos\n",
    "generated_sentiment_pos = np.linspace(1,1,10)\n",
    "lis_generated_sentence_neg = generate_synthetic_datasets(freq_dist_neg, kneser_ney_neg)\n",
    "generated_review_neg = lis_generated_sentence_neg\n",
    "generated_sentiment_neg = np.linspace(0,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4e518afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score is 0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the N-Gram Model we builtup in Step 2 on the synthetic data.\n",
    "\"\"\"\n",
    "\n",
    "generated_review = generated_review_pos + generated_review_neg\n",
    "generated_sentiment = [*generated_sentiment_pos, *generated_sentiment_neg] \n",
    "generated_predictions = model.predict(vect.transform(generated_review))\n",
    "for i in range(len(generated_predictions)):\n",
    "    if generated_predictions[i]=='positive':\n",
    "        generated_predictions[i]=1\n",
    "    elif generated_predictions[i]=='negative':\n",
    "        generated_predictions[i]=0\n",
    "print(\"AUC score is\", roc_auc_score(generated_sentiment, generated_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906be2a",
   "metadata": {},
   "source": [
    "### -> Accuracy of smoothing is very low, so we try Backoff as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa3ace",
   "metadata": {},
   "source": [
    "### 2. Backoff (with N=3)\n",
    "\n",
    "reference list:\n",
    "- https://github.com/AmrKhalifa/ML_DL-Tutorials/blob/master/NLP_Lab_sessions/lab2:%20N-gram%20Language%20Modeling/(solution).ipynb- \n",
    "- https://www.youtube.com/watch?v=gHC9tRyVSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "a745df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_x_pos = []\n",
    "for sent in x_pos:\n",
    "    l = list(sent.split())\n",
    "    lis_x_pos.append(l)\n",
    "lis_x_neg = []\n",
    "for sent in x_neg:\n",
    "    l = list(sent.split())\n",
    "    lis_x_neg.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "a01eb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram(data, n):\n",
    "    '''\n",
    "    Parameters:\n",
    "    data (list of element): each element is a sentence of the text \n",
    "    n (int): size of the n-gram\n",
    "    \n",
    "    Returns:\n",
    "    proba (dictionary of dictionary)\n",
    "    {\n",
    "        context: {word:probability of this word given context}\n",
    "    }\n",
    "    '''\n",
    "    total_number_words = 0\n",
    "    counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = tuple(sentence)\n",
    "        for i in range(len(sentence)):\n",
    "            total_number_words +=1\n",
    "            for k in range(n):\n",
    "                if i-k < 0:\n",
    "                    break\n",
    "                counts[sentence[i-k:i]][sentence[i]] +=1 \n",
    "\n",
    "    proba  = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    for context in counts.keys():\n",
    "        denom =0\n",
    "        for w in counts[context].keys():\n",
    "            denom += counts[context][w]\n",
    "        for w in counts[context].keys():\n",
    "            proba[context][w] = counts[context][w]/denom \n",
    "    \n",
    "    return proba\n",
    "\n",
    "\n",
    "def get_proba_distrib(model, context):\n",
    "    '''\n",
    "    Parameters: \n",
    "    model (dictionary of dictionary)\n",
    "    {\n",
    "        context: {word:probability of this word given context}\n",
    "    }\n",
    "    context (list of strings): the sentence we need to find the words after it and \n",
    "    thier probabilites\n",
    "    \n",
    "    Returns:\n",
    "    words_and_probs(dic): {word: probability of word given context}\n",
    "    \n",
    "    '''\n",
    "    # find the longest available ngram\n",
    "    if context in model:\n",
    "        return model[context]\n",
    "    else:\n",
    "        return get_proba_distrib(model, context[1:])\n",
    "    \n",
    "\n",
    "def generate(model):\n",
    "    '''\n",
    "    Parameters: \n",
    "    model (dictionary of dictionary)\n",
    "    {\n",
    "        context: {word:probability of this word given context}\n",
    "    }\n",
    "    \n",
    "    Returns:\n",
    "    sentence (list of strings): a sentence sampled according to the language model. \n",
    "    '''\n",
    "    sentence = [\"\"]\n",
    "    while sentence[-1] != \"eos\" and len(sentence)<100:\n",
    "        proba = get_proba_distrib(model, tuple(sentence))\n",
    "        w = np.random.choice((list(proba.keys())), 1, p = list(proba.values()))\n",
    "        sentence.append(w[0])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "3b63ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ngram model with n =  3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train N-gram model for positive and negative reviews, respectively. \n",
    "We choose n=3 in this case, since prediction accuracy is the highest when n=2.\n",
    "\"\"\"\n",
    "\n",
    "n = 3\n",
    "print(\"Build ngram model with n = \", n)\n",
    "model_pos = build_ngram(lis_x_pos, n)\n",
    "model_neg = build_ngram(lis_x_neg, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "b710e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic data (i.e., 500 new reviews for positive and negative respectively) according to the generative model in Step 2.\n",
    "\"\"\"\n",
    "\n",
    "generated_review_pos = []\n",
    "generated_review_neg = []\n",
    "for i in range(500):\n",
    "    generated_review_pos.append(generate(model_pos))\n",
    "    generated_review_neg.append(generate(model_neg))   \n",
    "generated_sentiment_pos = np.linspace(1,1,500)\n",
    "generated_sentiment_neg = np.linspace(0,0,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "fef3e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score is 0.8290000000000001\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the N-Gram Model we builtup in Step 2 on the synthetic data.\n",
    "\"\"\"\n",
    "\n",
    "generated_review = generated_review_pos + generated_review_neg\n",
    "generated_sentiment = [*generated_sentiment_pos, *generated_sentiment_neg] \n",
    "\n",
    "lis_generated_review = []\n",
    "for r in generated_review:\n",
    "    r_str = ' '.join(r) \n",
    "    lis_generated_review.append(r_str)\n",
    "generated_review = lis_generated_review \n",
    "generated_predictions = model[2].predict(vect[2].transform(generated_review))\n",
    "\n",
    "for i in range(len(generated_predictions)):\n",
    "    if generated_predictions[i]=='positive':\n",
    "        generated_predictions[i]=1\n",
    "    elif generated_predictions[i]=='negative':\n",
    "        generated_predictions[i]=0\n",
    "print(\"AUC score is\", roc_auc_score(generated_sentiment, generated_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "530d258b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sentiment = np.array(generated_sentiment)\n",
    "generated_sentiment = (np.rint(generated_sentiment)).astype(int)\n",
    "generated_predictions = generated_predictions.astype(str).astype(int)\n",
    "accuracy_score(generated_sentiment, generated_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb74c1",
   "metadata": {},
   "source": [
    "### -> Accuracy of backoff is pretty high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "27082d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the generated sentences as an excel.\n",
    "The first 500 sentences are positive, the last 500 sentences are negative. \n",
    "\"\"\"\n",
    "\n",
    "generated_review_df = pd.DataFrame(generated_review, columns=[\"generated reviews\"])\n",
    "generated_review_df.to_csv('ngram_synthetic_data_1000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99959545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
